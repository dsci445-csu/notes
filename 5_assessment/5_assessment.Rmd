---
title: "Chapter 5: Assessing Model Accuracy"
output:
  pagedown::html_paged:
    # change to true for a self-contained document, but it'll be a litte slower for Pandoc to render
    css: ["../style/my-style-page.css", "default-fonts", "default-page", "default"]
    self_contained: true
    number_sections: true
---

```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(scales)
library(ISLR)
library(knitr)
library(dplyr)
library(tidyr)
library(tidymodels)

opts_chunk$set(fig.height = 3, message = FALSE, warning = FALSE)
theme_set(theme_bw())

set.seed(445)
```

One of the key aims of this course is to introduce you to a wide range of statistical learning techniques. Why so many? Why not just the "best one"?

<br/><br/><br/><br/>

Hence, it's important to decide for any given set of data which method produces the best results.

<br/><br/>

![](https://imgs.xkcd.com/comics/machine_learning.png)

https://xkcd.com/1838/


# Measuring Quality of Fit

With linear regression we talked about some ways to measure fit of the model

<br/><br/><br/><br/><br/><br/>

In general, we need a way to measure fit and compare *across models*. 

<br/>

One way could be to measure how well its predictions match the observed data. In a regression session, the most commonly used measure is the *mean-squared error (MSE)*

<br/><br/><br/><br/><br/><br/>

We don't really care how well our methods work on the training data. 

<br/>

Instead, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen data. Why?

\newpage

So how do we select a method that minimizes the test MSE?

<br/><br/>

But what if we don't have a test set available?

<br/><br/><br/><br/>

```{r, echo = FALSE}
## generate training data
n <- 50
x <- runif(n, 0, 100)
f <- function(x) 20 + .0001*(-1*(x - 20) - 2*(x - 70)^2 - (x - 50)^3)
train <- data.frame(x = x, y = f(x) + rnorm(n, 0, 2))

## fit models of varying levels of flexibility
lm_spec <- linear_reg()
m0 <- lm_spec |> fit(y ~ x, data = train)
m1 <- workflow() |> add_recipe(recipe(y ~ x, data = train) |> step_ns(x, deg_free = 6)) |> add_model(lm_spec) |> fit(data = train)
m2 <- workflow() |> add_recipe(recipe(y ~ x, data = train) |> step_ns(x, deg_free = 25)) |> add_model(lm_spec) |> fit(data = train)

## get training MSE
m0 |> augment(new_data = train) |> mutate(model = "Linear Regression", df = 2) |>
  bind_rows(m1 |> augment(new_data = train) |> mutate(model = "Smoothing Spline", df = 6)) |>
  bind_rows(m2 |> augment(new_data = train) |> mutate(model = "Smoothing Spline", df = 25)) |>
  mutate(SE = (.pred - y)^2) |>
  group_by(model, df) |>
  summarise(`Train MSE` = mean(SE)) -> mse

## plot predictions vs. training data
ggplot() + 
  geom_point(aes(x, y), data = train) +
  geom_line(aes(x, f(x)), colour = "black") +
  geom_line(aes(x, .pred), colour = "blue", data = m0 |> augment(new_data = data.frame(x = seq(0, 100, length.out = 200)))) +
  geom_line(aes(x, .pred), colour = "red", data = m1 |> augment(new_data = data.frame(x = seq(0, 100, length.out = 200)))) +
  geom_line(aes(x, .pred), colour = "darkgreen", data = m2 |> augment(new_data = data.frame(x = seq(0, 100, length.out = 200))))

## generate test data
test <- data.frame(x = runif(1000, 0, 100))
test$y <- f(x) + rnorm(n, 0, 2)

## predictions for test data
m0.pred <- m0 |> augment(new_data = test)
m1.pred <- m1 |> augment(new_data = test)
m2.pred <- m2 |> augment(new_data = test)

## get test MSE
m0.pred |> mutate(model = "Linear Regression", df = 2) |>
  bind_rows(m1.pred |> mutate(model = "Smoothing Spline", df = 6)) |>
  bind_rows(m2.pred |> mutate(model = "Smoothing Spline", df = 25)) |>
  mutate(SE = (.pred - y)^2) %>%
  group_by(model, df) %>%
  summarise(`Test MSE` = mean(SE)) %>%
  left_join(mse) %>%
  knitr::kable(digits = 4) ## pretty table
```

\newpage

## Classification Setting

So far, we have talked about assessing model accuracy in the regression setting, but we also need a way to assess the accuracy of classification models.

Suppose we see to estimate $f$ on the basis of training observations where now the response is categorical. The most common approach for quantifying the accuracy is the training error rate.

<br/><br/><br/><br/><br/><br/>

This is called the *training error rate* because it is based on the data that was used to train the classifier. 

<br/>

As with the regression setting, we are mode interested in error rates for data *not* in our training data.

\newpage

## Bias-Variance Trade-off

The U-shape in the test MSE curve compared with flexibility is the result of two competing properties of statistical learning methods. It is possible to show that the expected test MSE, for a given test value $x_0$, can be decomposed

<br/><br/><br/><br/><br/><br/><br/><br/>

This tells us in order to minimize the expected test error, we need to select a statistical learning method that siulatenously achieves *low variance* and *low bias*.

<br/>

Variance -- <br/><br/><br/><br/>

Bias -- <br/><br/><br/><br/>

\newpage

# Cross-Validation

As we have seen, the test error can be easily calculated when there is a test data set available. 

<br/><br/><br/><br/>

In contrast, the training error can be easily calculated.

<br/><br/><br/><br/>

In the absense of a very large designated test set that can be used to estimate the test error rate, what to do?

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

For now we will assume we are in the regression setting (quantitative response), but concepts are the same for classification.

\newpage

## Validation Set

Suppose we would like to estimate the test error rate for a particular statistical learning method on a set of observations. What is the easiest thing we can think to do?

<br/><br/><br/><br/><br/><br/><br/>

Let's do this using the `mpg` data set. Recall we found a non-linear relationship between `displ` and `hwy` mpg.

```{r, echo = FALSE}
ggplot(mpg) +
  geom_point(aes(displ, hwy)) +
  geom_smooth(aes(displ, hwy))
```

<br/><br/>

We fit the model with a squared term $\texttt{displ}^2$, but we might be wondering if we can get better predictive performance by including higher power terms!

\newpage

```{r}
## get index of training observations
# take 60% of observations as training and 40% for validation
n <- nrow(mpg)
trn <- seq_len(n) %in% sample(seq_len(n), round(0.6*n)) 

## fit models
m0 <- lm(hwy ~ displ, data = mpg[trn, ])
m1 <- lm(hwy ~ displ + I(displ^2), data = mpg[trn, ])
m2 <- lm(hwy ~ displ + I(displ^2) + I(displ^3), data = mpg[trn, ])
m3 <- lm(hwy ~ displ + I(displ^2) + I(displ^3) + I(displ^4), data = mpg[trn, ])

## predict on validation set
pred0 <- predict(m0, mpg[!trn, ])
pred1 <- predict(m1, mpg[!trn, ])
pred2 <- predict(m2, mpg[!trn, ])
pred3 <- predict(m3, mpg[!trn, ])

## estimate test MSE
true_hwy <-  mpg[!trn, ]$hwy # truth vector

data.frame(terms = 2, model = "linear", true = true_hwy, pred = pred0) %>%
  bind_rows(data.frame(terms = 3, model = "quadratic", true = true_hwy, pred = pred1)) %>%
  bind_rows(data.frame(terms = 4, model = "cubic", true = true_hwy, pred = pred2)) %>%
  bind_rows(data.frame(terms = 5, model = "quartic", true = true_hwy, pred = pred3)) %>% ## bind predictions together
  mutate(se = (true - pred)^2) %>% # squared errors
  group_by(terms, model) %>% # group by model
  summarise(test_mse = mean(se)) %>% ## get test mse
  kable() ## pretty table
```

\newpage

```{r, echo = FALSE, message = FALSE}
res <- data.frame() ## store results

for(i in 1:10) { # repeat 10 times
  trn <- seq_len(n) %in% sample(seq_len(n), round(0.9*n)) 

  ## fit models
  m0 <- lm(hwy ~ displ, data = mpg[trn, ])
  m1 <- lm(hwy ~ displ + I(displ^2), data = mpg[trn, ])
  m2 <- lm(hwy ~ displ + I(displ^2) + I(displ^3), data = mpg[trn, ])
  m3 <- lm(hwy ~ displ + I(displ^2) + I(displ^3) + I(displ^4), data = mpg[trn, ])
  
  ## predict on validation set
  pred0 <- predict(m0, mpg[!trn, ])
  pred1 <- predict(m1, mpg[!trn, ])
  pred2 <- predict(m2, mpg[!trn, ])
  pred3 <- predict(m3, mpg[!trn, ])
  
  ## estimate test MSE
  data.frame(iter = i, terms = 2, model = "linear", true = mpg[!trn, ]$hwy, pred = pred0) %>%
    bind_rows(data.frame(iter = i, terms = 3, model = "quadratic", true = mpg[!trn, ]$hwy, pred = pred1)) %>%
    bind_rows(data.frame(iter = i, terms = 4, model = "cubic", true = mpg[!trn, ]$hwy, pred = pred2)) %>%
    bind_rows(data.frame(iter = i, terms = 5, model = "quartic", true = mpg[!trn, ]$hwy, pred = pred3)) %>% ## bind predictions together
    mutate(se = (true - pred)^2) %>% # squared errors
    group_by(iter, terms, model) %>% # group by model
    summarise(test_mse = mean(se)) -> test_mse
  
  res %>%
    bind_rows(test_mse) -> res
}

res %>%
  mutate(iter = as.character(iter)) %>%
  ggplot() +
  geom_line(aes(terms, test_mse, group = iter, colour = iter)) +
  theme(legend.position = "none")
```

\newpage

## Leave-One-Out Cross Validation

*Leave-one-out cross-validation* (LOOCV) is closely related to the validation set approach, but it attempts to address the method's drawbacks.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

The LOOCV estimate for the test MSE is

<br/><br/><br/><br/>

LOOCV has a couple major advantages and a few disadvantages.

\newpage

```{r, message = FALSE}
## perform LOOCV on the mpg dataset
res <- data.frame() ## store results
for(i in seq_len(n)) { # repeat for each observation
  trn <- seq_len(n) != i # leave one out

  ## fit models
  m0 <- lm(hwy ~ displ, data = mpg[trn, ])
  m1 <- lm(hwy ~ displ + I(displ^2), data = mpg[trn, ])
  m2 <- lm(hwy ~ displ + I(displ^2) + I(displ^3), data = mpg[trn, ])
  m3 <- lm(hwy ~ displ + I(displ^2) + I(displ^3) + I(displ^4), data = mpg[trn, ])
  
  ## predict on validation set
  pred0 <- predict(m0, mpg[!trn, ])
  pred1 <- predict(m1, mpg[!trn, ])
  pred2 <- predict(m2, mpg[!trn, ])
  pred3 <- predict(m3, mpg[!trn, ])
  
  ## estimate test MSE
  true_hwy <- mpg[!trn, ]$hwy # get truth vector
  
  res %>% ## store results for use outside the loop
    bind_rows(data.frame(terms = 2, model = "linear", true = true_hwy, pred = pred0)) %>%
    bind_rows(data.frame(terms = 3, model = "quadratic", true = true_hwy, pred = pred1)) %>%
    bind_rows(data.frame(terms = 4, model = "cubic", true = true_hwy, pred = pred2)) %>%
    bind_rows(data.frame(terms = 5, model = "quartic", true = true_hwy, pred = pred3)) %>% ## bind predictions together
    mutate(mse = (true - pred)^2) -> res
}

res %>%
  group_by(terms, model) %>%
  summarise(LOOCV_test_MSE = mean(mse)) %>%
  kable()
```

## k-Fold Cross Validation

An alternative to LOOCV is $k$-fold CV.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

The $k$-fold CV estimate is computed by averaging

<br/><br/><br/><br/><br/><br/>

Why $k$-fold over LOOCV?

\newpage

```{r}
## perform k-fold on the mpg dataset
res <- data.frame() ## store results

## get the folds
k <- 10
folds <- sample(seq_len(10), n, replace = TRUE) ## approximately equal sized

for(i in seq_len(k)) { # repeat for each observation
  trn <- folds != i # leave ith fold out

  ## fit models
  m0 <- lm(hwy ~ displ, data = mpg[trn, ])
  m1 <- lm(hwy ~ displ + I(displ^2), data = mpg[trn, ])
  m2 <- lm(hwy ~ displ + I(displ^2) + I(displ^3), data = mpg[trn, ])
  m3 <- lm(hwy ~ displ + I(displ^2) + I(displ^3) + I(displ^4), data = mpg[trn, ])
  
  ## predict on validation set
  pred0 <- predict(m0, mpg[!trn, ])
  pred1 <- predict(m1, mpg[!trn, ])
  pred2 <- predict(m2, mpg[!trn, ])
  pred3 <- predict(m3, mpg[!trn, ])
  
  ## estimate test MSE
  true_hwy <- mpg[!trn, ]$hwy # get truth vector
  
  data.frame(terms = 2, model = "linear", true = true_hwy, pred = pred0) %>%
    bind_rows(data.frame(terms = 3, model = "quadratic", true = true_hwy, pred = pred1)) %>%
    bind_rows(data.frame(terms = 4, model = "cubic", true = true_hwy, pred = pred2)) %>%
    bind_rows(data.frame(terms = 5, model = "quartic", true = true_hwy, pred = pred3)) %>% ## bind predictions together
    mutate(mse = (true - pred)^2) %>%
    group_by(terms, model) %>%
    summarise(mse = mean(mse)) -> test_mse_k
  
  res %>% bind_rows(test_mse_k) -> res
}




res %>%
  group_by(terms, model) %>%
  summarise(kfoldCV_test_MSE = mean(mse)) %>%
  kable()
```

<br/><br/><br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE, message = FALSE, cache = TRUE}
## repear k-fold on the mpg dataset 10x
res_cv <- data.frame() ## store results
k <- 10

for(j in 1:10) {
  res <- data.frame()
  ## get the folds
  folds <- sample(seq_len(10), n, replace = TRUE) ## approximately equal sized
  
  for(i in seq_len(k)) { # repeat for each observation
    trn <- folds != i # leave ith fold out
  
    ## fit models
    m0 <- lm(hwy ~ displ, data = mpg[trn, ])
    m1 <- lm(hwy ~ displ + I(displ^2), data = mpg[trn, ])
    m2 <- lm(hwy ~ displ + I(displ^2) + I(displ^3), data = mpg[trn, ])
    m3 <- lm(hwy ~ displ + I(displ^2) + I(displ^3) + I(displ^4), data = mpg[trn, ])
    
    ## predict on validation set
    pred0 <- predict(m0, mpg[!trn, ])
    pred1 <- predict(m1, mpg[!trn, ])
    pred2 <- predict(m2, mpg[!trn, ])
    pred3 <- predict(m3, mpg[!trn, ])
    
    ## estimate test MSE
    true_hwy <- mpg[!trn, ]$hwy # get truth vector
    
    data.frame(terms = 2, model = "linear", true = true_hwy, pred = pred0) %>%
      bind_rows(data.frame(terms = 3, model = "quadratic", true = true_hwy, pred = pred1)) %>%
      bind_rows(data.frame(terms = 4, model = "cubic", true = true_hwy, pred = pred2)) %>%
      bind_rows(data.frame(terms = 5, model = "quartic", true = true_hwy, pred = pred3)) %>% ## bind predictions together
      mutate(mse = (true - pred)^2) %>%
      group_by(terms, model) %>%
      summarise(mse = mean(mse)) -> test_mse_k
    
    res %>% bind_rows(test_mse_k) -> res
  }
  
  res %>%
    mutate(iter = j) %>%
    group_by(iter, terms, model) %>%
    summarise(kfoldCV_test_MSE = mean(mse)) %>%
    bind_rows(res_cv) -> res_cv
}

res_cv %>%
  mutate(iter = as.character(iter)) %>%
  ggplot() +
  geom_line(aes(terms, kfoldCV_test_MSE, group = iter, colour = iter)) +
  theme(legend.position = "none")
```

\newpage

## Bias-Variance Trade-off for $k$-Fold Cross Validation

$k$-Fold CV with $k < n$ has a computational advantace to LOOCV.

<br/><br/><br/><br/>

We know the validation approach can overestimate the test error because we use only half of the data to fit the statistical learning method.

<br/><br/><br/><br/><br/><br/><br/><br/>


But we know that bias is only half the story! We also need to consider the procedure's variance.


<br/><br/><br/><br/><br/><br/><br/><br/>

To summarise, there is a bias-variance trade-off associated with the choice of $k$ in $k$-fold CV. Typically we use $k = 5$ or $k = 10$ because these have been shown empirically to  yield test error rates closest to the truth.

\newpage

## Cross-Validation for Classification Problems

So far we have talked only about CV for regression problems.

<br/><br/><br/><br/>

But CV can also be very useful for classification problems! For example, the LOOCV error rate for classification problems takes the form

<br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE}
library(class)
library(mvtnorm)
```

```{r, echo = FALSE, cache = TRUE, fig.show='hold', out.width="50%", fig.height=5}
mu_1 <- c(-1, 1)
mu_2 <- c(2, -1)
mu_3 <- c(-3, .1)
mu_4 <- c(1, -1)
sigma_1 <- matrix(c(1, .5, .5, 1), nrow = 2)
sigma_2 <- matrix(c(1, -.5, -.5, 1), nrow = 2)

sample_mixture2 <- function(n, mu_1, mu_2, sigma, p) {
  z <- rbinom(n, 1, p)
  z*rmvnorm(n, mu_1, sigma) + (1 - z)*rmvnorm(n, mu_2, sigma) 
}

d_mixture2 <- function(x, mu_1, mu_2, sigma, p) {
  p*dmvnorm(x, mu_1, sigma) + (1 - p)*dmvnorm(x, mu_2, sigma) 
}

# training data from the mixture
train <- data.frame(class = "1", sample_mixture2(100, mu_1, mu_2, sigma_1, 0.2)) %>%
  bind_rows(data.frame(class = "2", sample_mixture2(100, mu_3, mu_4, sigma_1, 0.7)))

names(train) <- c("class", "x1", "x2")

bayes_classifier <- function(x) {
  as.character(as.numeric(d_mixture2(x, mu_1, mu_2, sigma_1, 0.2)*0.5 < d_mixture2(x, mu_3, mu_3, sigma_1, 0.7)*0.5) + 1)
}

## create plot data to separate the space
expand.grid(x1 = seq(-6, 6, length.out = 100), 
            x2 = seq(-6, 6, length.out = 100)) %>%
  data.frame() -> plot_dat

## knn plots for each k
for(k in c(1, 10, 100)) {
  ## fit knn
  knn_fit <- knn(train[, -1], plot_dat, train$class, k = k)
  
  knn_plot_dat <- plot_dat %>%
    mutate(class = as.character(knn_fit))
  
  ## make plots
  knn_plot_dat %>%
    ggplot() +
    geom_tile(aes(x1, x2, fill = class), alpha = 0.5) +
    geom_point(aes(x1, x2, colour = class), data = train) +
    ggtitle(paste("KNN, K =", k)) +
    theme(text = element_text(size = 20), legend.position = 'hide') -> p
  
  print(p)
}

plot_dat %>%
  mutate(class = apply(plot_dat, 1, bayes_classifier)) %>%
  ggplot() +
  geom_tile(aes(x1, x2, fill = class), alpha = 0.5) +
  geom_point(aes(x1, x2, colour = class), data = train) +
  ggtitle("Bayes Classifier") +
  theme(text = element_text(size = 20))
```

```{r}
k_fold <- 10
cv_label <- sample(seq_len(k_fold), nrow(train), replace = TRUE)
err <- rep(NA, k) # store errors for each flexibility level

for(k in seq(1, 100, by = 2)) {
  err_cv <- rep(NA, k_fold) # store error rates for each fold
  for(ell in seq_len(k_fold)) {
    trn_vec <- cv_label != ell # fit model on these
    tst_vec <- cv_label == ell # estimate error on these
    
    ## fit knn
    knn_fit <- knn(train[trn_vec, -1], train[tst_vec, -1], train[trn_vec, ]$class, k = k)
    ## error rate
    err_cv[ell] <- mean(knn_fit != train[tst_vec, ]$class)
  }
  err[k] <- mean(err_cv)
}
err <- na.omit(err)
```

```{r, echo = FALSE}
ggplot() +
  geom_line(aes(seq(1, 100, by = 2), err)) +
  geom_point(aes(seq(1, 100, by = 2)[which.min(err)], err[which.min(err)]), shape = 3) +
  xlab("KNN K") + ylab("K-Fold CV Error")
```

<br/><br/><br/>

Minimum CV error of `r round(err[which.min(err)], 4)` found at $K = `r seq(1, 100, by = 2)[which.min(err)]`$.
