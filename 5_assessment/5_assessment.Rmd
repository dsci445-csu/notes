---
title: "Chapter 5: Assessing Model Accuracy"
output:
  pagedown::html_paged:
    # change to true for a self-contained document, but it'll be a litte slower for Pandoc to render
    css: ["../style/my-style-page.css", "default-fonts", "default-page", "default"]
    self_contained: true
    number_sections: true
---

```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(scales)
library(ISLR)
library(knitr)
library(dplyr)
library(tidyr)
library(tidymodels)

opts_chunk$set(fig.height = 3, message = FALSE, warning = FALSE)
theme_set(theme_bw())

set.seed(445)
```

One of the key aims of this course is to introduce you to a wide range of statistical learning techniques. Why so many? Why not just the "best one"?

<br/><br/><br/><br/>

Hence, it's important to decide for any given set of data which method produces the best results.

<br/><br/>

![](https://imgs.xkcd.com/comics/machine_learning.png)

https://xkcd.com/1838/


# Measuring Quality of Fit

With linear regression we talked about some ways to measure fit of the model

<br/><br/><br/><br/><br/><br/>

In general, we need a way to measure fit and compare *across models*. 

<br/>

One way could be to measure how well its predictions match the observed data. In a regression session, the most commonly used measure is the *mean-squared error (MSE)*

<br/><br/><br/><br/><br/><br/>

We don't really care how well our methods work on the training data. 

<br/>

Instead, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen data. Why?

\newpage

So how do we select a method that minimizes the test MSE?

<br/><br/>

But what if we don't have a test set available?

<br/><br/><br/><br/>

```{r, echo = FALSE}
## generate training data
n <- 50
x <- runif(n, 0, 100)
f <- function(x) 20 + .0001*(-1*(x - 20) - 2*(x - 70)^2 - (x - 50)^3)
train <- data.frame(x = x, y = f(x) + rnorm(n, 0, 2))

## fit models of varying levels of flexibility
lm_spec <- linear_reg()
m0 <- lm_spec |> fit(y ~ x, data = train)
m1 <- workflow() |> add_recipe(recipe(y ~ x, data = train) |> step_ns(x, deg_free = 6)) |> add_model(lm_spec) |> fit(data = train)
m2 <- workflow() |> add_recipe(recipe(y ~ x, data = train) |> step_ns(x, deg_free = 25)) |> add_model(lm_spec) |> fit(data = train)

## get training MSE
m0 |> augment(new_data = train) |> mutate(model = "Linear Regression", df = 2) |>
  bind_rows(m1 |> augment(new_data = train) |> mutate(model = "Smoothing Spline", df = 6)) |>
  bind_rows(m2 |> augment(new_data = train) |> mutate(model = "Smoothing Spline", df = 25)) |>
  mutate(SE = (.pred - y)^2) |>
  group_by(model, df) |>
  summarise(`Train MSE` = mean(SE)) -> mse

## plot predictions vs. training data
ggplot() + 
  geom_point(aes(x, y), data = train) +
  geom_line(aes(x, f(x)), colour = "black") +
  geom_line(aes(x, .pred), colour = "blue", data = m0 |> augment(new_data = data.frame(x = seq(0, 100, length.out = 200)))) +
  geom_line(aes(x, .pred), colour = "red", data = m1 |> augment(new_data = data.frame(x = seq(0, 100, length.out = 200)))) +
  geom_line(aes(x, .pred), colour = "darkgreen", data = m2 |> augment(new_data = data.frame(x = seq(0, 100, length.out = 200))))

## generate test data
test <- data.frame(x = runif(1000, 0, 100))
test$y <- f(x) + rnorm(n, 0, 2)

## predictions for test data
m0.pred <- m0 |> augment(new_data = test)
m1.pred <- m1 |> augment(new_data = test)
m2.pred <- m2 |> augment(new_data = test)

## get test MSE
m0.pred |> mutate(model = "Linear Regression", df = 2) |>
  bind_rows(m1.pred |> mutate(model = "Smoothing Spline", df = 6)) |>
  bind_rows(m2.pred |> mutate(model = "Smoothing Spline", df = 25)) |>
  mutate(SE = (.pred - y)^2) %>%
  group_by(model, df) %>%
  summarise(`Test MSE` = mean(SE)) %>%
  left_join(mse) %>%
  knitr::kable(digits = 4) ## pretty table
```

\newpage

## Classification Setting

So far, we have talked about assessing model accuracy in the regression setting, but we also need a way to assess the accuracy of classification models.

Suppose we see to estimate $f$ on the basis of training observations where now the response is categorical. The most common approach for quantifying the accuracy is the training error rate.

<br/><br/><br/><br/><br/><br/>

This is called the *training error rate* because it is based on the data that was used to train the classifier. 

<br/>

As with the regression setting, we are mode interested in error rates for data *not* in our training data.

\newpage

## Bias-Variance Trade-off

The U-shape in the test MSE curve compared with flexibility is the result of two competing properties of statistical learning methods. It is possible to show that the expected test MSE, for a given test value $x_0$, can be decomposed

<br/><br/><br/><br/><br/><br/><br/><br/>

This tells us in order to minimize the expected test error, we need to select a statistical learning method that siulatenously achieves *low variance* and *low bias*.

<br/>

Variance -- <br/><br/><br/><br/>

Bias -- <br/><br/><br/><br/>

\newpage

# Cross-Validation

As we have seen, the test error can be easily calculated when there is a test data set available. 

<br/><br/><br/><br/>

In contrast, the training error can be easily calculated.

<br/><br/><br/><br/>

In the absense of a very large designated test set that can be used to estimate the test error rate, what to do?

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

For now we will assume we are in the regression setting (quantitative response), but concepts are the same for classification.

\newpage

## Validation Set

Suppose we would like to estimate the test error rate for a particular statistical learning method on a set of observations. What is the easiest thing we can think to do?

<br/><br/><br/><br/><br/><br/><br/>

Let's do this using the `mpg` data set. Recall we found a non-linear relationship between `displ` and `hwy` mpg.

```{r, echo = FALSE}
ggplot(mpg) +
  geom_point(aes(displ, hwy)) +
  geom_smooth(aes(displ, hwy))
```

<br/><br/>

We fit the model with a squared term $\texttt{displ}^2$, but we might be wondering if we can get better predictive performance by including higher power terms!

\newpage

```{r}
## get index of training observations
# take 60% of observations as training and 40% for validation
mpg_val <- validation_split(mpg, prop = 0.6)

## models
lm_spec <- linear_reg()

linear_recipe <- recipe(hwy ~ displ, data = mpg)
quad_recipe <- linear_recipe |> step_mutate(displ2 = displ^2)
cubic_recipe <- quad_recipe |> step_mutate(displ3 = displ^3)
quart_recipe <- cubic_recipe |> step_mutate(displ4 = displ^4)

m0 <- workflow() |> add_model(lm_spec) |> add_recipe(linear_recipe) |> fit_resamples(resamples = mpg_val)
m1 <- workflow() |> add_model(lm_spec) |> add_recipe(quad_recipe) |> fit_resamples(resamples = mpg_val)
m2 <- workflow() |> add_model(lm_spec) |> add_recipe(cubic_recipe) |> fit_resamples(resamples = mpg_val)
m3 <- workflow() |> add_model(lm_spec) |> add_recipe(quart_recipe) |> fit_resamples(resamples = mpg_val)

## estimate test MSE
collect_metrics(m0) |> mutate(model = "linear") |>
  bind_rows(collect_metrics(m1) |> mutate(model = "quadratic")) |>
  bind_rows(collect_metrics(m2) |> mutate(model = "cubic")) |>
  bind_rows(collect_metrics(m3) |> mutate(model = "quartic")) |> 
  select(model, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  select(-rsq) |>
  kable()
```

\newpage

```{r, echo = FALSE, message = FALSE}
res <- data.frame() ## store results

for(i in 1:10) { # repeat 10 times
  mpg_val <- validation_split(mpg, prop = 0.6)

  m0 <- workflow() |> add_model(lm_spec) |> add_recipe(linear_recipe) |> fit_resamples(resamples = mpg_val)
  m1 <- workflow() |> add_model(lm_spec) |> add_recipe(quad_recipe) |> fit_resamples(resamples = mpg_val)
  m2 <- workflow() |> add_model(lm_spec) |> add_recipe(cubic_recipe) |> fit_resamples(resamples = mpg_val)
  m3 <- workflow() |> add_model(lm_spec) |> add_recipe(quart_recipe) |> fit_resamples(resamples = mpg_val)

## estimate test MSE
collect_metrics(m0) |> mutate(model = "linear", terms = 2) |>
  bind_rows(collect_metrics(m1) |> mutate(model = "quadratic", terms = 3)) |>
  bind_rows(collect_metrics(m2) |> mutate(model = "cubic", terms = 4)) |>
  bind_rows(collect_metrics(m3) |> mutate(model = "quartic", terms = 5)) |> 
  select(model, terms, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  select(-rsq) |>
  mutate(iter = i) |>
  bind_rows(res) -> res
}

res %>%
  mutate(iter = as.character(iter)) %>%
  ggplot() +
  geom_line(aes(terms, rmse, group = iter, colour = iter)) +
  theme(legend.position = "none")
```

\newpage

## Leave-One-Out Cross Validation

*Leave-one-out cross-validation* (LOOCV) is closely related to the validation set approach, but it attempts to address the method's drawbacks.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

The LOOCV estimate for the test MSE is

<br/><br/><br/><br/>

LOOCV has a couple major advantages and a few disadvantages.

\newpage

```{r, message = FALSE, cache = TRUE}
## perform LOOCV on the mpg dataset
mpg_loocv <- vfold_cv(mpg, v = nrow(mpg))

## models
m0 <- workflow() |> add_model(lm_spec) |> add_recipe(linear_recipe) |> fit_resamples(resamples = mpg_loocv)
m1 <- workflow() |> add_model(lm_spec) |> add_recipe(quad_recipe) |> fit_resamples(resamples = mpg_loocv)
m2 <- workflow() |> add_model(lm_spec) |> add_recipe(cubic_recipe) |> fit_resamples(resamples = mpg_loocv)
m3 <- workflow() |> add_model(lm_spec) |> add_recipe(quart_recipe) |> fit_resamples(resamples = mpg_loocv)

## estimate test MSE
collect_metrics(m0) |> mutate(model = "linear") |>
  bind_rows(collect_metrics(m1) |> mutate(model = "quadratic")) |>
  bind_rows(collect_metrics(m2) |> mutate(model = "cubic")) |>
  bind_rows(collect_metrics(m3) |> mutate(model = "quartic")) |> 
  select(model, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  select(-rsq) |>
  kable()
```

## k-Fold Cross Validation

An alternative to LOOCV is $k$-fold CV.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

The $k$-fold CV estimate is computed by averaging

<br/><br/><br/><br/><br/><br/>

Why $k$-fold over LOOCV?

\newpage

```{r, cache = TRUE}
## perform k-fold on the mpg dataset
mpg_10foldcv <- vfold_cv(mpg, v = 10)

## models
m0 <- workflow() |> add_model(lm_spec) |> add_recipe(linear_recipe) |> fit_resamples(resamples = mpg_10foldcv)
m1 <- workflow() |> add_model(lm_spec) |> add_recipe(quad_recipe) |> fit_resamples(resamples = mpg_10foldcv)
m2 <- workflow() |> add_model(lm_spec) |> add_recipe(cubic_recipe) |> fit_resamples(resamples = mpg_10foldcv)
m3 <- workflow() |> add_model(lm_spec) |> add_recipe(quart_recipe) |> fit_resamples(resamples = mpg_10foldcv)

## estimate test MSE
collect_metrics(m0) |> mutate(model = "linear") |>
  bind_rows(collect_metrics(m1) |> mutate(model = "quadratic")) |>
  bind_rows(collect_metrics(m2) |> mutate(model = "cubic")) |>
  bind_rows(collect_metrics(m3) |> mutate(model = "quartic")) |> 
  select(model, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  select(-rsq) |>
  kable()
```

<br/><br/><br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE, message = FALSE, cache = TRUE}
## repear k-fold on the mpg dataset 10x
res_cv <- data.frame() ## store results

for(i in 1:10) { # repeat 10 times
  ## perform k-fold on the mpg dataset
  mpg_10foldcv <- vfold_cv(mpg, v = 10)
  
  ## models
  m0 <- workflow() |> add_model(lm_spec) |> add_recipe(linear_recipe) |> fit_resamples(resamples = mpg_10foldcv)
  m1 <- workflow() |> add_model(lm_spec) |> add_recipe(quad_recipe) |> fit_resamples(resamples = mpg_10foldcv)
  m2 <- workflow() |> add_model(lm_spec) |> add_recipe(cubic_recipe) |> fit_resamples(resamples = mpg_10foldcv)
  m3 <- workflow() |> add_model(lm_spec) |> add_recipe(quart_recipe) |> fit_resamples(resamples = mpg_10foldcv)

## estimate test MSE
collect_metrics(m0) |> mutate(model = "linear", terms = 2) |>
  bind_rows(collect_metrics(m1) |> mutate(model = "quadratic", terms = 3)) |>
  bind_rows(collect_metrics(m2) |> mutate(model = "cubic", terms = 4)) |>
  bind_rows(collect_metrics(m3) |> mutate(model = "quartic", terms = 5)) |> 
  select(model, terms, .metric, mean) |>
  pivot_wider(names_from = .metric, values_from = mean) |>
  select(-rsq) |>
  mutate(iter = i) |>
  bind_rows(res_cv) -> res_cv
}

res_cv %>%
  mutate(iter = as.character(iter)) %>%
  ggplot() +
  geom_line(aes(terms, rmse, group = iter, colour = iter)) +
  theme(legend.position = "none")
```

\newpage

## Bias-Variance Trade-off for $k$-Fold Cross Validation

$k$-Fold CV with $k < n$ has a computational advantace to LOOCV.

<br/><br/><br/><br/>

We know the validation approach can overestimate the test error because we use only half of the data to fit the statistical learning method.

<br/><br/><br/><br/><br/><br/><br/><br/>


But we know that bias is only half the story! We also need to consider the procedure's variance.


<br/><br/><br/><br/><br/><br/><br/><br/>

To summarise, there is a bias-variance trade-off associated with the choice of $k$ in $k$-fold CV. Typically we use $k = 5$ or $k = 10$ because these have been shown empirically to  yield test error rates closest to the truth.

\newpage

## Cross-Validation for Classification Problems

So far we have talked only about CV for regression problems.

<br/><br/><br/><br/>

But CV can also be very useful for classification problems! For example, the LOOCV error rate for classification problems takes the form

<br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE}
library(mvtnorm)
```

```{r, echo = FALSE, cache = TRUE, fig.height=5}
mu_1 <- c(-1, 1)
mu_2 <- c(2, -1)
mu_3 <- c(-3, .1)
mu_4 <- c(1, -1)
sigma_1 <- matrix(c(1, .5, .5, 1), nrow = 2)
sigma_2 <- matrix(c(1, -.5, -.5, 1), nrow = 2)

sample_mixture2 <- function(n, mu_1, mu_2, sigma, p) {
  z <- rbinom(n, 1, p)
  z*rmvnorm(n, mu_1, sigma) + (1 - z)*rmvnorm(n, mu_2, sigma) 
}

d_mixture2 <- function(x, mu_1, mu_2, sigma, p) {
  p*dmvnorm(x, mu_1, sigma) + (1 - p)*dmvnorm(x, mu_2, sigma) 
}

# training data from the mixture
train <- data.frame(class = "1", sample_mixture2(100, mu_1, mu_2, sigma_1, 0.2)) |>
  bind_rows(data.frame(class = "2", sample_mixture2(100, mu_3, mu_4, sigma_1, 0.7))) |>
  mutate(class = as.factor(class))

names(train) <- c("class", "x1", "x2")

bayes_classifier <- function(x) {
  as.character(as.numeric(d_mixture2(x, mu_1, mu_2, sigma_1, 0.2)*0.5 < d_mixture2(x, mu_3, mu_3, sigma_1, 0.7)*0.5) + 1)
}

## create plot data to separate the space
expand.grid(x1 = seq(-6, 6, length.out = 100), 
            x2 = seq(-6, 6, length.out = 100),
            class = "1") |>
  data.frame() -> plot_dat

all_dat <- bind_rows(train |> mutate(type = "train"), plot_dat |> mutate(type = "plot"))

grid_small <- tibble(neighbors = c(1, 10, 100))
train_rs <- validation_time_split(all_dat, prop = nrow(train)/nrow(all_dat))

## knn plots for each k
knn_spec <- nearest_neighbor(mode = "classification", neighbors = tune("neighbors"))
knn_control <- control_resamples(save_pred = TRUE)
knn_spec |>
  tune_grid(class ~ x1 + x2, resamples = train_rs, grid = grid_small, control = knn_control) -> knn_fit

knn_fit |> augment(parameters = data.frame(neighbors = 1)) |> filter(type == "plot") |> select(x1, x2, .pred_class) |> mutate(model = "KNN, K = 1") |>
  bind_rows(knn_fit |> augment(parameters = data.frame(neighbors = 10)) |> filter(type == "plot") |> select(x1, x2, .pred_class) |> mutate(model = "KNN, K = 10")) |>
  bind_rows(knn_fit |> augment(parameters = data.frame(neighbors = 100)) |> filter(type == "plot") |> select(x1, x2, .pred_class) |> mutate(model = "KNN, K = 100")) |>
  bind_rows(plot_dat[, -3] |> mutate(.pred_class = apply(plot_dat[, -3], 1, bayes_classifier), model = "Bayes Classifier")) |>
  ggplot() +
  geom_tile(aes(x1, x2, fill = .pred_class), alpha = 0.5) +
  geom_point(aes(x1, x2, colour = class), data = train) +
  facet_wrap(~model) +
  theme(text = element_text(size = 20))
```

```{r}
k_fold <- 10
train_cv <- vfold_cv(train, v = k_fold)

grid_large <- tibble(neighbors = seq(1, 100, by = 2))

knn_spec <- nearest_neighbor(mode = "classification", neighbors = tune("neighbors"))
knn_spec |>
  tune_grid(class ~ x1 + x2, resamples = train_cv, grid = grid_large) |>
  collect_metrics() |>
  filter(.metric == "accuracy") |>
  mutate(error = 1 - mean) -> knn_err
```

```{r, echo = FALSE}
ggplot() +
  geom_line(aes(neighbors, error), knn_err) +
  geom_point(aes(neighbors, error), data = knn_err |> filter(error == min(error)), shape = 3) +
  xlab("KNN K") + ylab("K-Fold CV Error")
```

<br/><br/><br/>

Minimum CV error of `r round(knn_err |> filter(error == min(error)) |> pull(error) |> min(), 4)` found at $K = `r knn_err |> filter(error == min(error)) |> pull(neighbors) |> min()`$.
