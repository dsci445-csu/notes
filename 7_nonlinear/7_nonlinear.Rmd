---
title: "Chapter 7: Moving Beyond Linarity"
output:
  pagedown::html_paged:
    # change to true for a self-contained document, but it'll be a litte slower for Pandoc to render
    css: ["../style/my-style-page.css", "default-fonts", "default-page", "default"]
    self_contained: true
    number_sections: true
---

```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(scales)
library(ISLR)
library(knitr)
library(dplyr)
library(tidyr)
library(tidymodels)

opts_chunk$set(fig.height = 3, message = FALSE, warning = FALSE)
theme_set(theme_bw())

set.seed(445)
```

So far we have mainly focused on linear models.

<br/><br/><br/><br/><br/><br/>

Previously, we have seen we can improve upon least squares using ride regression, the lasso, principal components regression, and more.

<br/>

Through simple and more sophisticated extensions of the linear model, we can relax the linearity assumption while still maintiaining as much interpretability as possible.


# Step Functions

Using polynomial functions of the features as predictors imposes a *global* structure on the non-linear function of $X$.

<br/>

We can instead use *step-functions* to avoid imposing a global structure.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

For a given value of $X$, at most one of $C_1, \dots, C_K$ can be non-zero.

\newpage

Example: Wage data.

```{r, echo = FALSE}
head(Wage, 4) %>% knitr::kable(row.names = FALSE)
```
<br/><br/><br/>
```{r, echo = FALSE, fig.show='hold', out.width='50%', fig.height=6}
df <- Wage[, c("age", "wage")] |>
  mutate(c1 = as.numeric(age >= 30 & age < 60),
         c2 = as.numeric(age >= 60)) |>
  select(-age)

lm_spec <- linear_reg()

lm_spec |>
  fit(wage ~ ., data = df) -> m0

interval <- predict(m0, new_data = df, type = "conf_int")

m0 |>
  augment(new_data = df) |>
  ggplot() +
  geom_point(aes(age, wage), alpha = .3, data = Wage) +
  geom_ribbon(aes(x = Wage$age, ymin = interval$.pred_lower, ymax = interval$.pred_upper), fill = "red", alpha = .2) +
  geom_line(aes(Wage$age, .pred), colour = "red", size = 2) +
  theme(text = element_text(size = 30))
  
df2 <- Wage[, c("age", "wage")] |>
  mutate(c1 = as.numeric(age >= 30 & age < 60),
         c3 = as.numeric(age >= 60),
         high_wage = wage > 250) |>
  mutate(high_wage = factor(high_wage)) |>
  select(-age, -wage)

logistic_spec <- logistic_reg()

m1 <- logistic_spec |>
  fit(high_wage ~ ., data = df2)

interval <- predict(m1, new_data = df2, type = "conf_int")

m1 |>
  augment(new_data = df2) |>
  ggplot() +
  geom_ribbon(aes(x = Wage$age, ymin = interval$.pred_lower_TRUE, ymax = interval$.pred_upper_TRUE), fill = "red", alpha = .2) +
  geom_line(aes(Wage$age, .pred_TRUE), colour = "red", size = 2) +
  xlab("age") +
  ylab("Pr(wage > 250 | age)") +
  theme(text = element_text(size = 30))
```

\newpage

# Basis Functions

Polynomial and piecewise-constant regression models are in face special cases of a *basis function approach*.

**Idea:** <br/><br/><br/><br/>

Instead of fitting the linear model in $X$, we fit the model

<br/><br/><br/><br/>

Note that the basis functions are fixed and known.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

We can think of this model as a standard linear model with predictors defined by the basis functions and use least squares to estimate the unknown regression coefficients.

<br/><br/>

# Regression Splines

*Regression splines* are a very common choice for basis function because they are quite flexible, but still interpretable. Regression splines extend upon polynomial regression and piecewise constant approaches seen previously.

## Piecewise Polynomials

Instead of fitting a high degree polynomial over the entire range of $X$, piecewise polynomial regression involves fitting separate low-degree polynomials over different regions of $X$.

<br/><br/><br/><br/><br/><br/>

For example, a pieacewise cubic with no knots is just a standard cubic polynomial. 

<br/><br/>

A pieacewise cubic with a single knot at point $c$ takes the form

<br/><br/><br/><br/><br/><br/><br/><br/>

Using more knots leads to a more flexible piecewise polynomial.

<br/><br/><br/>

In general, we place  $K$ knots throughout the range of $X$ and fit $K + 1$ polynomial regression models.

\newpage

## Constraints and Splines

To avoid having too much flexibility, we can *constrain* the piecewise polynomial so that the fitted curve must be continuous.

<br/><br/><br/>

To go further, we could add two more constraints

<br/><br/><br/>

In other words, we are requiring the piecewise polynomials to be *smooth*.

<br/><br/><br/>

Each constraint that we impose on the piecewise cubic polynomials effectively frees up one degree of freedom, bu reducing the complexity of the resulting fit.

<br/>

The fit with continuity and 2 smoothness contraints is called a *spline*.

<br/>

A degree-$d$ spline is <br/><br/>

```{r, fig.show='hold', out.width='33%', fig.height=5, echo = FALSE}
Wage_small <- Wage[sample(1:nrow(Wage), 100),]

df3 <- Wage_small[, c("age", "wage")] %>%
  mutate(age_low = age*(age < 50),
         age_high = age*(age >= 50)) %>%
  select(-age)

cubic_spline_rec <- recipe(wage ~ age, data = Wage_small) |>
  step_bs(age, degree = 3, options = list(knots = c(50)))

cts_piecewise_cubic_rec <- recipe(wage ~ age_low + age_high, data = df3) |>
  step_poly(starts_with("age"), degree = 3)

piecewise_cubic_rec <- recipe(wage ~ age, data = Wage_small) |>
  step_poly(age, degree = 3)

spline_cubic <- workflow() |>
  add_model(lm_spec) |>
  add_recipe(cubic_spline_rec) |>
  fit(data = Wage_small)

cts_piecewise_cubic <- workflow() |>
  add_model(lm_spec) |>
  add_recipe(cts_piecewise_cubic_rec) |>
  fit(data = df3)

piecewise_cubic1 <- workflow() |>
  add_model(lm_spec) |>
  add_recipe(piecewise_cubic_rec) |>
  fit(data = Wage_small %>% filter(age < 50))
piecewise_cubic2 <- workflow() |>
  add_model(lm_spec) |>
  add_recipe(piecewise_cubic_rec) |>
  fit(data = Wage_small %>% filter(age >= 50))

x1s <- data.frame(age = seq(min(Wage_small$age), 50))
x2s <- data.frame(age = seq(50, max(Wage_small$age)))
y1s <- predict(piecewise_cubic1, new_data = x1s)
y2s <- predict(piecewise_cubic2, new_data = x2s)

ggplot() +
  geom_point(aes(age, wage), alpha = .3, data = Wage_small) +
  geom_line(aes(x1s$age, y1s$.pred), colour = "red", size = 2) +
  geom_line(aes(x2s$age, y2s$.pred), colour = "red", size = 2) +
  geom_vline(aes(xintercept = 50), lty = 2) +
  theme(text = element_text(size = 30))

cts_piecewise_cubic |>
  augment(new_data = df3) |>
  ggplot() +
  geom_point(aes(age, wage), alpha = .3, data = Wage_small) +
  geom_line(aes(Wage_small$age, .pred), colour = "red", size = 2) +
  geom_vline(aes(xintercept = 50), lty = 2) +
  theme(text = element_text(size = 30))

spline_cubic |>
  augment(new_data = Wage_small) |>
  ggplot() +
  geom_point(aes(age, wage), alpha = .3) +
  geom_line(aes(age, .pred), colour = "red", size = 2) +
  geom_vline(aes(xintercept = 50), lty = 2) +
  theme(text = element_text(size = 30))

```

\newpage

## Spline Basis Representation

Fitting the spline regression model is more complex than the piecewise polynomial regression. We need to fit a degree $d$ piecewise polynomial and also constrain it and it's $d - 1$ derivatives to be continuous at the knots.

<br/><br/><br/><br/><br/>

The most direct way to represent a cubic spline is to start with the basis for a cubic polynomial and add one *truncated power basis* function per knot.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

Unfortunately, splines can have high variance at the outer range of the predictors. One solution is to add *boundary contraints*.

\newpage


## Choosing the Knots

When we fit a spline, where should we place the knots?

<br/><br/><br/><br/><br/><br/><br/>

How many knots should we use?

<br/><br/><br/><br/><br/><br/><br/>

## Comparison to Polynomial Regression

<br/><br/><br/>

```{r, echo = FALSE}
spline_15_rec <- recipe(wage ~ age, data = Wage) |>
  step_ns(age, deg_free = 15)

poly_15_rec <- recipe(wage ~ age, data = Wage) |>
  step_poly(age, degree = 15)


spline_fit <- workflow() |>
  add_model(lm_spec) |>
  add_recipe(spline_15_rec) |>
  fit(data = Wage)
  
poly_fit <- workflow() |>
  add_model(lm_spec) |>
  add_recipe(poly_15_rec) |>
  fit(data = Wage)

xs <- data.frame(age = seq(16, 81))

spline_fit |>
  augment(new_data = xs) |>
  rename(.pred_spline = .pred) |>
  left_join(poly_fit |> augment(new_data = xs)) |>
  ggplot() +
  geom_point(aes(age, wage), alpha = .3, data = Wage) +
  geom_line(aes(age, .pred_spline), colour = "red") +
  geom_line(aes(age, .pred), colour = "blue")
```


# Generalized Additive Models

So far we have talked about flexible ways to predict $Y$ based on a single predictor $X$.

<br/><br/><br/><br/>

*Generalized Additive Models (GAMs)* provide a general framework for extending a standard linear regression model by allowing non-linear functions of each of the variables while maintaining *additivity*.

<br/><br/><br/>

## GAMs for Regression

A natural way to extend the multiple linear regression model to allow for non-linear relationships between feature and response:

\newpage

The beauty of GAMs is that we can use our fitting ideas in this chapter as building blocks for fitting an additive model.

Example: Consider the Wage data.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE, fig.show='hold', out.width='33%', fig.height = 6}
# plot function
plot_f <- function(fit) {
  preplot_obj <- gam:::preplot.Gam(fit)
  for(var in names(preplot_obj)) {
    obj <- preplot_obj[[var]]
    if(class(obj$x) %in% c("integer", "numeric")) {
      p <- ggplot() +
        geom_ribbon(aes(obj$x, ymin = obj$y - 1.96*obj$se.y, ymax = obj$y + 1.96*obj$se.y), alpha = 0.2) +
        geom_line(aes(obj$x, obj$y)) +
        xlab(obj$xlab) + ylab(obj$ylab) +
        theme(text = element_text(size = 30))
    } else if(class(obj$x) %in% c("factor", "character")) {
      p <- ggplot() +
        geom_errorbar(aes(obj$x, ymin = obj$y - 1.96*obj$se.y, ymax = obj$y + 1.96*obj$se.y), alpha = 0.8) +
        geom_point(aes(obj$x, obj$y)) +
        xlab(obj$xlab) + ylab(obj$ylab) +
        theme(text = element_text(size = 30), axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
    }
    print(p)
  }
}


plot_gam <- function(wf_fit, ns_x_vars, raw_x_vars) {
  fit <- extract_fit_parsnip(wf_fit)
  rec <- extract_preprocessor(wf_fit)
  
  df <- rec$template
  
  
}
gam_rec <- recipe(wage ~ age + year + education, data = Wage) |>
  step_ns(year, deg_free = 4) |>
  step_ns(age, deg_free = 5)

workflow() |>
  add_model(lm_spec) |>
  add_recipe(gam_rec) |>
  fit(data = Wage) -> gam_fit

gam_fit |>
  augment(new_data = Wage) -> gam_pred

gam_rec |> 
  prep() |> 
  bake(new_data = Wage) |> 
  select(starts_with("age")) |>
  as.matrix() -> col_ns_mat

tidy(gam_fit) |>
  select(estimate, term) |>
  pivot_wider(names_from = term, values_from = estimate) |>
  as.matrix() -> gam_ests

tidy(gam_fit) |>
  mutate(ci_lower = estimate - 2*std.error) |>
  select(term, ci_lower) |>
  pivot_wider(names_from = term, values_from = ci_lower) |>
  as.matrix() -> gam_ests_lower

tidy(gam_fit) |>
  mutate(ci_upper = estimate + 2*std.error) |>
  select(term, ci_upper) |>
  pivot_wider(names_from = term, values_from = ci_upper) |>
  as.matrix() -> gam_ests_upper

age_ns <- age_ns_mat %*% gam_ests[grepl("age", colnames(gam_ests))]
age_ns_lower <- age_ns_mat %*% gam_ests_lower[grepl("age", colnames(gam_ests_lower))]
age_ns_upper <- age_ns_mat %*% gam_ests_lower[grepl("age", colnames(gam_ests_upper))]


ggplot() +
  

```

\newpage

Pros and Cons of GAMs

\newpage

## GAMs for Classification

GAMs can also be used in situations where $Y$ is categorical. Recall the logistic regression model:

<br/><br/><br/><br/><br/><br/>

A natural way to extend this model is for non-linear relationships to be used.

<br/><br/><br/><br/><br/><br/>

Example: Consider the Wage data.

<br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE, fig.show='hold', out.width='33%', fig.height = 6}
gam_fit_logit <- gam(I(wage > 250) ~ ns(year, df = 4) + ns(age, df = 5) + education, data = Wage, family = "binomial")
plot_f(gam_fit_logit)
```




