---
title: "Chapter 3: Linear Regression"
output:
  pagedown::html_paged:
    # change to true for a self-contained document, but it'll be a litte slower for Pandoc to render
    css: ["../style/my-style-page.css", "default-fonts", "default-page", "default"]
    self_contained: true
    number_sections: true
---

```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(scales)
library(ISLR)
library(knitr)

opts_chunk$set(fig.height = 3, message = FALSE, warning = FALSE)
theme_set(theme_bw())
```

*Linear regression* is a simple approach for supervised learning when the response is quantitative. Linear regression has a long history and we could actually spend most of this semester talking about it.

Although linear regression is not the newest, shiniest thing out there, it is still a highly used technique out in the real world. It is also useful for talking about more modern techniques that are **generalizations** of it.

We will review some key ideas underlying linear regression and discuss the least squares approach that is most commonly used to fit this model.

Linear regression can help us to answer the following questions about our `Advertising` data:

\newpage

# Simple Linear Regression

*Simple Linear Regression* is an approach for predictiong a quantitative response $Y$ on the basis of a single predictor variable $X$.

It assumes:

<br/><br/><br/><br/><br/><br/>

Which leads to the following model:

<br/><br/><br/><br/>

For example, we may be interested in regressing `sales` onto `TV` by fitting the model

<br/><br/><br/><br/>

Once we have used training data to produce estimates $\hat{\beta}_0$ and $\hat{\beta}_1$, we can predict future sales on the basis of a particular TV advertising budget.

<br/><br/><br/><br/><br/>

## Estimating the Coefficients

In practice, $\beta_0$ and $\beta_1$ are **unknown**, so before we can predict $\hat{y}$, we must use our training data to estimate them.

Let $(x_1, y_1), \dots, (x_n, y_n)$ represent $n$ observation pairs, each of which consists of a measurement of $X$ and $Y$.

<br/><br/><br/>

**Goal:** Obtain coefficient estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ such that the linear model fits the available data well.

<br/><br/>

The most common approach involves minimizing the *least squares* criterion.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

The least squares approach results in the following estimates:

$$
\hat{\beta}_1 = \\
\hat{\beta}_0 = 
$$

We can get these estimates using the following commands in `R` and `tidymodels`:

```{r}
library(tidymodels) ## load library

## load the data in
ads <- read_csv("../data/Advertising.csv", col_select = -1) 

## fit the model
lm_spec <- linear_reg() |>
  set_mode("regression") |>
  set_engine("lm")

slr_fit <- lm_spec |>
  fit(sales ~ TV, data = ads)

slr_fit |>
  pluck("fit") |>
  summary()
```

```{r, echo=FALSE, eval=FALSE}
ggplot(aes(TV, sales), data = ads) +
  geom_point() +
  geom_smooth(method = "lm")
```

<br/><br/>

## Assessing Accuracy

Recall we assume the *true* relationship between $X$ and $Y$ takes the form

<br/><br/>

If $f$ is to be approximated by a linear function, we can write this relationship as

<br/><br/>

and when we fit the model to the training data, we get the following estimate of the *population model*

<br/><br/>

But how close this this to the truth?

<br/><br/><br/><br/><br/>

In general, $\sigma^2$ is not known, so we estimate it with the *residual standard error*, $RSE = \sqrt{RSS/(n - 2)}$.

We can use these standard errors to compute confidence intervals and perform hypothesis tests.

\newpage

Once we have decided that there is a significant linear relationship between $X$ and $Y$ that is captured by our model, it is natural to ask

> To what extent does the model fit the data?

The quality of the fit is usually measured by the *residual standard error* and the $R^2$ statistic.

**RSE**: Roughly speaking, the RSE is the average amount that the response will deviate from the true regression line. This is considered a measure of the *lack of fit* of the model to the data.

$R^2$: The RSE provides an absolute measure of lack of fit, but is measured in the units of $Y$. So, we don't know what a "good" RSE value is! $R^2$ gives the proportion of variation in $Y$ explained by the model.

```{r}
slr_fit |>
  pluck("fit") |>
  summary()
```

# Multiple Linear Regression

Simple linear regression is useful for predicting a response based on one predictor variable, but we often have **more than one** predictor.

> How can we extend our approach to accommodate additional predictors?

<br/><br/><br/><br/><br/>

We can give each predictor a separate slope coefficient in a single model.

<br/><br/><br/><br/>

We interpret $\beta_j$ as the "average effect on $Y$ of a one unit increase in $X_j$, *holding all other predictors fixed*".

In our `Advertising` example, 

\newpage

## Estimating the Coefficients

As with the case of simple linear regression, the coefficients $\beta_0, \beta_1, \dots, \beta_p$ are unknown and must be estimated. Given estimates $\hat{\beta}_0, \hat{\beta}_1, \dots, \hat{\beta}_p$, we can make predictions using the formula

<br/><br/>

The parameters are again estimated using the same least squares approach that we saw in the context of simple linear regression.

```{r}
# mlr_fit <- lm_spec |> fit(sales ~ TV + radio + newspaper, data = ads) 
mlr_fit <- lm_spec |> 
  fit(sales ~ ., data = ads) 

mlr_fit |>
  pluck("fit") |>
  summary()
```


## Some Important Questions

When we perform multiple linear regression we are usually interested in answering a few important questions:

1. <br/><br/><br/>

1. <br/><br/><br/>

1. <br/><br/><br/>

1. <br/><br/><br/>

### Is there a relationship between response and predictors?

We need to ask whether all of the regression coefficients are zero, which leads to the following hypothesis test.

$$
H_0: \qquad\\
~\\
H_a: \qquad
$$

This hypothesis test is performed by computing the $F$-statistic

$$
F = \qquad
$$

\newpage

### Deciding on Important Variables

After we have computed the $F$-statistic and concluded that there is a relationship between predictor and response, it is natural to wonder

> Which predictors are related to the response?

We could look at the $p$-values on the individual coefficients, but if we have many variables this can lead to false discoveries.

Instead we could consider *variable selection*. We will revisit this in Ch. 6.

### Model Fit

Two of the most common measures of model fit are the RSE and $R^2$. These quantities are computed and interpreted in the same way as for simple linear regression.

Be careful with using these alone, because $R^2$ will **always increase** as more variables are added to the model, even if it's just a small increase.

```{r}
# model with TV, radio, and newspaper
mlr_fit |> pluck("fit") |> summary()
```

```{r}
# model without newspaper
lm_spec |> fit(sales ~ TV + radio, data = ads) |>
  pluck("fit") |> summary()
```

It may also be useful to plot residuals to get a sense of the model fit.

```{r, fig.height=2}
ggplot() +
  geom_point(aes(mlr_fit$fit$fitted.values, mlr_fit$fit$residuals))
```

# Other Considerations

## Categorical Predictors

So far we have assumed all variables in our linear model are quantitiative.

For example, consider building a model to predict highway gas mileage from the `mpg` data set.

```{r}
head(mpg)
```

```{r, message = FALSE, fig.height=9, cache=TRUE}
library(GGally)

mpg %>% 
  select(-model) %>% # too many models
  ggpairs() # plot matrix
```

\newpage

To incorporate these categorical variables into the model, we will need to introduce $k - 1$ dummy variables, where $k =$ the number of levels in the variable, for each qualitative variable.

For example, for `drv`, we have 3 levels: `4`, `f`, and `r`. 

<br/><br/><br/><br/><br/><br/>

```{r}
lm_spec |>
  fit(hwy ~ displ + cty + drv, data = mpg) |>
  pluck("fit") |>
  summary()
```

\newpage

## Extensions of the Model

The standard regression model provides interpretable results and works well in many problems. However it makes some very strong assumptions that may not always be reasonable.

<br/>

**Additive Assumption**

The additive assumption assumes that the effect of each predictor on the response is not affected by the value of the other predictors. What if we think the effect should depend on the value of another predictor?

```{r}
lm_spec |>
  fit(sales ~ TV + radio + TV*radio, data = ads) |>
  pluck("fit") |>
  summary()
```


\newpage

**Linearity Assumption**

The linear regression model assumes a linear relationship between response and predictors. In some cases, the true relationship may be non-linear.

```{r}
ggplot(data = mpg, aes(displ, hwy)) +
  geom_point() +
  geom_smooth(method = "lm", colour = "red") +
  geom_smooth(method = "loess", colour = "blue")
```

\newpage

```{r}
lm_spec |>
  fit(hwy ~ displ + I(displ^2), data = mpg) |>
  pluck("fit") |> summary()
```

## Potential Problems

1. Non-linearity of response-predictor relationships <br/><br/><br/><br/>

1. Correlation of error terms <br/><br/><br/><br/>

1. Non-constant variance of error terms <br/><br/><br/><br/>

1. Outliers 

\newpage

# $K$-Nearest Neighbors

In Ch. 2 we discuss the differences between *parametric* and *nonparametric* methods. Linear regression is a parametric method because it assumes a linear functional form for $f(X)$.

<br/>

A simple and well-known non-parametric method for regression is called $K$-nearest neighbors regression (KNN regression).

Given a value for $K$ and a prediction point $x_0$, KNN regression first identifies the $K$ training observations that are closest to $x_0$ ($\mathcal{N}_0$). It then estimates $f(x_0)$ using the average of all the training responses in $\mathcal{N}_0$,

<br/>

```{r, fig.show="hold", out.width = "33%", fig.height = 7}
set.seed(445) #reproducibility

## generate data
x <- rnorm(100, 4, 1) # pick some x values
y <- 0.5 + x + 2*x^2 + rnorm(100, 0, 2) # true relationship
df <- data.frame(x = x, y = y) # data frame of training data

knn_spec <- nearest_neighbor(mode = "regression")
for (k in seq(2, 10, by = 2)) {
  knn_spec |>
    fit(y ~ x, data = df, neighbors = k) |>
    augment(new_data = df) |>
    ggplot() +
    geom_point(aes(x, y)) +
    geom_line(aes(x, .pred), colour = "red") +
    ggtitle(paste("KNN, k = ", k)) +
    theme(text = element_text(size = 30)) -> p
    
  print(p)
}

lm_spec |>
  fit(y ~ x, df) |>
  augment(new_data = df) |>
  ggplot() +
    geom_point(aes(x, y)) +
    geom_line(aes(x, .pred), colour = "red") +
    ggtitle("Simple Linear Regression") +
    theme(text = element_text(size = 30)) # slr plot

```

