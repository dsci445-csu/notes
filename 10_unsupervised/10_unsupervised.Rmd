---
title: "Chapter 10: Unsupervised Learning"
output:
  pagedown::html_paged:
    # change to true for a self-contained document, but it'll be a litte slower for Pandoc to render
    css: ["../style/my-style-page.css", "default-fonts", "default-page", "default"]
    self_contained: true
    number_sections: true
---

```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(scales)
library(ISLR)
library(knitr)
library(dplyr)
library(tidyr)
library(dendextend)
library(tidymodels)


opts_chunk$set(fig.height = 3, message = FALSE, warning = FALSE)
theme_set(theme_bw())

set.seed(445)
```


![](https://thejenkinscomic.net/images/366.png)

Credit: https://thejenkinscomic.net/?id=366

This chapter will focus on methods intended for the setting in which we only have a set of features $X_1, \dots, X_p$ measured on $n$ observations.

\newpage

# The Challenge of Unsupervised Learning


Supervised learning is a well-understood area.

<br/><br/><br/><br/><br/><br/><br/><br/>

In contrast, unsupervised learning is often much more challenging.

<br/><br/><br/><br/>

Unsupervised learning is often performed as part of an *exploratory data analysis*.

<br/>

It can be hard to assess the results obtained from unsupervised learning methods.

<br/><br/><br/><br/><br/><br/><br/><br/>

Techniques for unsupervised learning are of growing importance in a number of fields.

\newpage

# Principal Components Analysis

We have already seen principal components as a method for dimension reduction.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

*Principal Components Analysis (PCA)* refers to the process by which principal components are computed and the subsequent use of these components to understand the data.

<br/><br/><br/><br/>

Apart from producing derived variables forr use in supervised learning, PCA also serves as a tool for data visualization.

\newpage

## What are Principal Components?

Suppose we wish to visualize $n$ observations with measurements on a set of $p$ features as part of an exploratory data analysis.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

**Goal:** We would like to find a low-dimensional representation of the data that captures as much of the information as possible.

<br/><br/><br/><br/><br/>

PCA provides us a tool to do just this.

<br/><br/><br/>

**Idea:** Each of the $n$ observations lives in $p$ dimensional space, but not all of these dimensions are equally interesting.

\newpage

The *first principal component* of a set of features $X_1, \dots, X_p$ is the normalized linear combination of the features 

<br/><br/><br/><br/><br/><br/><br/><br/>

that has the largest variance.

<br/>

Given a $n \times p$ data set $\boldsymbol X$, how do we compute the first principal component?

\newpage

There is a nice geometric interpretation for the first principal component.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

After the first principal component $Z_1$ of the features has been determined, we can find the second principal component, $Z_2$. The second principal component is the linear combination of $X_1, \dots, X_p$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_1$.

\newpage

Once we have computed the principal components, we can plot them against each other to produce low-dimensional views of the data. 

<br/>

```{r, fig.height = 6}
str(USArrests)

USArrests_pca <- USArrests |>
  prcomp(scale = TRUE, center = TRUE)

summary(USArrests_pca) # summary

tidy(USArrests_pca, matrix = "loadings") |> # principal components loading matrix
  pivot_wider(names_from = PC, values_from = value)

## plot scores + directions
biplot(USArrests_pca)
```

\newpage

## Scaling Variables

We've already talked about how when PCA is performed, the varriables should be centered to have mean zero. 

<br/><br/><br/><br/>

This is in contrast to other methods we've seen before.

<br/><br/><br/>

```{r, fig.height = 5.5, echo = FALSE}
USArrests_pca_unscaled <- USArrests |>
  prcomp(scale = FALSE, center = TRUE)

## plot scores + directions
biplot(USArrests_pca_unscaled)
```

\newpage

## Uniqueness

Each principal component loading vector is unique, up to a sign flip.

<br/><br/><br/><br/><br/><br/>

Similarly, the score vectors are unique up to a sign flip.

<br/><br/><br/><br/>

## Proportion of Variance Explained

We have seen using the `USArrests` data that e can summarize $50$ observations in $4$ dimensions using just the first two principal component score vectors and the first two principal component vectors.

**Question:** <br/><br/><br/><br/>

More generally, we are interested in knowing the *proportion of vriance explained (PVE)* by each principal component.

\newpage

## How Many Principal Components to Use

In general, a $n \times p$ matrix $\boldsymbol X$ has $\min(n - 1, p)$ distinct principal components.

<br/><br/>

Rather, we would like to just use the first few principal components in order to visualize or interpret the data.

<br/><br/><br/><br/>

We typically decide on the number of principal components required by examining a *scree plot*.

<br/><br/>

```{r, echo = FALSE}
tidy(USArrests_pca, matrix = "eigenvalues") |>
  ggplot(aes(PC, percent)) +
  geom_line() +
  geom_point() +
  ylab("PVE")
```

\newpage

## Other Uses for Principal Components

We've seen previously that we can perform regression using the principal component score vectors as features for dimension reduction.

<br/><br/><br/><br/><br/><br/>

Many statistical techniques can be easily adapted to use the $n \times M$ matrix whose columns are the first $M << p$ principal components.

<br/><br/><br/><br/>

This can lead to *less noisy* results.

\newpage

# Clustering

*Clustering* refers to a broad set of techniques for finding *subgroups* in a data set.

<br/><br/><br/><br/><br/><br/><br/><br/>

For instance, suppose we have a set of $n$ observations, each with $p$ features. The $n$ observations could correspond to tissue samples for patients with breast cancer and the $p$ features could correspond to

<br/><br/><br/><br/><br/><br/><br/><br/>

We may have reason to believe there is heterogeneity among the $n$ observations.

<br/><br/><br/><br/><br/><br/>

This is *unsupervised* because

\newpage

Both clustering and PCA seek to simplify the data via a small number of summaries.

<br/>

- PCA <br/><br/><br/>

- Clustering <br/><br/><br/>

Since clustering is popular in many fields, there are many ways to cluster.

<br/><br/><br/>

- $K$-means clustering <br/><br/><br/><br/><br/><br/>

- Hierarchical clustering <br/><br/><br/><br/><br/><br/>

In general, we can cluster observations on the basis of features or we can cluster features on the basis of observations.

\newpage





## K-Means Clustering

Simple and elegant approach to parition a data set into $K$ distinct, non-overlapping clusters.

<br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE, fig.show='hold', out.width="33%", fig.height = 6}
X <- mvtnorm::rmvnorm(100, mean = c(0, 0))
X[1:33,] <- X[1:33,] + matrix(rep(c(-3, -3), 33), ncol = 2, byrow = TRUE)
X[34:67,] <- X[34:67,] + matrix(rep(c(3, 0), 34), ncol = 2, byrow = TRUE)

K2 <- kmeans(X, 2)
K3 <- kmeans(X, 3)
K4 <- kmeans(X, 4)

data.frame(X) |>
  ggplot() +
  geom_point(aes(X1, X2, colour = as.factor(K2$cluster)), size = 2) +
  theme(legend.position = "none", text = element_text(size = 30)) +
  ggtitle("K = 2") +
  xlab("") + ylab("")

data.frame(X) |>
  ggplot() +
  geom_point(aes(X1, X2, colour = as.factor(K3$cluster)), size = 2) +
  theme(legend.position = "none", text = element_text(size = 30)) +
  ggtitle("K = 3") +
  xlab("") + ylab("")

data.frame(X) |>
  ggplot() +
  geom_point(aes(X1, X2, colour = as.factor(K4$cluster)), size = 2) +
  theme(legend.position = "none", text = element_text(size = 30)) +
  ggtitle("K = 4") +
  xlab("") + ylab("")
```

<br/>

The $K$-means clustering procedure results from a simple and intuitive mathematical problem. Let $C_1, \dots, C_K$ denote sets containing the indices of observations in each cluster. These satisfy two properties:

<br/>

1. <br/><br/><br/><br/>

2. <br/><br/><br/><br/>

**Idea:** 

\newpage

The *within-cluster variation* for cluster $C_k$ is a measure of the amount by which the observations within a cluster differ from each other.

<br/><br/><br/><br/><br/><br/>

To solve this, we need to define within-cluster variation.

<br/><br/><br/><br/><br/><br/>

This results in the following optimization problem that defines $K$-means clustering:

<br/><br/><br/><br/><br/><br/>

A very simple algorithm has been shown to find a local optimum to this problem:

\newpage

## Hierarchical Clustering

One potential disadvantage of $K$-means clustering is that it requires us to specify the number of clusters $K$. *Hierarchical clustering* is an alternative that does not require we commit to a particular $K$.

<br/><br/><br/><br/>

We will discuss *bottom-up* or *agglomerative* clustering.

<br/><br/><br/><br/>

### Dendrograms


<br/>

```{r, echo = FALSE, fig.show='hold', out.width="50%", fig.height = 6}
data.frame(X) |>
  ggplot() +
  geom_point(aes(X1, X2), size = 2) +
  theme(text = element_text(size = 30)) +
  xlab("") + ylab("")

hc <- dist(X) |>
  hclust()
dend <- as.dendrogram(hc)

plot(dend)
```

\newpage

Each *leaf* of the dendrogram represents one of the $100$ simulated data points.

<br/>

As we move up the tree, leaves begin to fuse into branches, which correspond to observations that are similar to each other.

<br/><br/><br/><br/><br/>

For any two observations, we can look for the point in the tree where branches containing those two observations are first fused.


<br/><br/><br/><br/><br/>

How do we get clusters from the dendrogram?

<br/>

```{r, echo = FALSE, fig.show='hold', out.width="50%", fig.height = 6}
plot(color_labels(dend, k = 2))
abline(h = 9, lty = 2)

plot(color_labels(dend, k = 3))
abline(h = 5, lty = 2)
```

\newpage

The term *hierarchical* refers to the fact that clusters obtained by cutting the dendrogram at a given height are necessarily nested within the clusters obtained by cutting the dendrogram at a greater height.

<br/><br/><br/><br/><br/><br/>

### Algorithm

First, we need to define some sort of *dissimilarity* metric between pairs of observations.

<br/><br/><br/>

Then the algorithm proceeds iteratively.

<br/><br/>

```{r, echo = FALSE, fig.show='hold', out.width="50%", fig.height = 6}
X_small <- mvtnorm::rmvnorm(10, c(0, 0))

data.frame(X_small) |>
  ggplot() +
  geom_text(aes(X1, X2, label = 1:10), size = 4) +
  theme(text = element_text(size = 30)) +
  xlab("") + ylab("")

plot(hclust(dist(X_small)))
```

\newpage

More formally,

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

One issue has not yet been addressed.

<br/><br/><br/>

How do we determine the dissimilarity between two clusters if one or both of them contains multiple observations?

<br/><br/><br/><br/><br/>

1. <br/><br/><br/>

2. <br/><br/><br/>

3. <br/><br/><br/>

4. 

\newpage

### Choice of Dissimilarity Metric

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

## Practical Considerations in Clustering

In order to perform clustering, some decisions should be made.

<br/>

- <br/><br/><br/>

- <br/><br/><br/><br/><br/><br/>

- <br/><br/><br/>

Each of these decisions can have a strong impact on the results obtained. What to do?




