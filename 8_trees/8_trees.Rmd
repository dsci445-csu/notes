---
title: "Chapter 8: Tree-Based Methods"
output:
  pagedown::html_paged:
    # change to true for a self-contained document, but it'll be a litte slower for Pandoc to render
    css: ["../style/my-style-page.css", "default-fonts", "default-page", "default"]
    self_contained: true
    number_sections: true
---

```{r echo = FALSE, message = FALSE}
library(tidyverse)
library(scales)
library(ISLR)
library(knitr)
library(dplyr)
library(tidyr)
library(tidymodels)
library(rpart.plot)

opts_chunk$set(fig.height = 3, message = FALSE, warning = FALSE)
theme_set(theme_bw())

set.seed(445)
```

We will introduce *tree-based* methods for regression and classification.

<br/><br/><br/><br/><br/><br/>

The set of splitting rules can be summarized in a tree $\Rightarrow$ "decision trees".

<br/><br/><br/><br/><br/><br/>

Combining a large number of trees can often result in dramatic improvements in prediction accuracy at the expense of interpretation.


![](http://www.phdcomics.com/comics/archive/phd042007s.gif)

Credit: http://phdcomics.com/comics.php?f=852

Decision trees can be applied to both regression and classification problems. We will start with regression.

# Regression Trees

**Example:** We want to predict baseball salaries using the `Hittters` data set based on `Years` (the number of years that a player has been in the major leagues) and `Hits` (the number of hits he made the previous year).

<br/><br/><br/><br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE, fig.height=4}
ggplot(Hitters) +
  geom_point(aes(Years, Hits))
```

The predicted salary for players is given by the mean response value for the players in that box. Overall, the tree segments the players into 3 regions of predictor space.

\newpage

We now discuss the process of building a regression tree. There are to steps:

<br/>

1. <br/><br/><br/><br/><br/><br/>

2. <br/><br/><br/><br/><br/><br/>

<br/>

How do we construct the regions $R_1, \dots, R_J$?

<br/><br/><br/>

The goal is to find boxes $R_1, \dots, R_J$ that minimize the RSS.

<br/><br/><br/><br/><br/>

The approach is *top-down* because

<br/><br/><br/><br/><br/>

The approach is *greedy* because

\newpage

In order to perform recursive binary splitting,

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

The process described above may produce good predictions on the training set, but is likely to overfit the data.

<br/>

A smaller tree, with less splits might lead to lower variance and better interpretation at the cost of a little bias.

<br/><br/><br/><br/>

A strategy is to grow a very large tree $T_0$ and then *prune* it back to obtain a *subtree*.

\newpage

Algorithm for building a regression tree:

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

```{r, echo = FALSE, fig.show='hold', out.width="33%", fig.height = 6, message = FALSE}
tree_spec <- decision_tree() %>%
  set_engine("rpart")

reg_tree_spec <- tree_spec %>%
  set_mode("regression")

## make validation set
Hitters_split <- initial_split(Hitters)

Hitters_train <- training(Hitters_split)
Hitters_test <- testing(Hitters_split)

## fit regression tree
reg_tree_fit <- fit(reg_tree_spec, Salary ~ ., Hitters_train)

reg_tree_fit |>
  extract_fit_engine() |>
  rpart.plot()

## cross validation
reg_tree_wf <- workflow() |>
  add_model(reg_tree_spec |> set_args(tree_depth = tune())) |>
  add_formula(Salary ~ .)

Hitters_fold <- vfold_cv(Hitters_train, v = 10)

param_grid <- grid_regular(tree_depth(range = c(1, 8)), levels = 8)

tune_res <- reg_tree_wf |>
  tune_grid(resamples = Hitters_fold, grid = param_grid)

autoplot(tune_res, metric = "rmse")

## pruned by CV
best_depth <- select_best(tune_res, metric = "rmse")
reg_tree_final <- finalize_workflow(reg_tree_wf, data.frame(tree_depth = 1.6))
reg_tree_final_fit <- fit(reg_tree_final, data = Hitters_train)

reg_tree_final_fit |>
  extract_fit_engine() |>
  rpart.plot()
```

\newpage

# Classification Trees

A *classification tree* is very similar to a regression tree, except that it is used to predict a categorical response.

<br/><br/><br/>

For a classification tree, we predict that each observation belongs to the *most commonly occurring class* of training observation in the region to which it belongs.

<br/><br/><br/><br/><br/><br/>

The task of growing a classification tree is quite similar to the task of growing a regression tree.

<br/><br/><br/><br/><br/><br/>

It turns out that classification error is not sensitive enough.

<br/><br/><br/><br/><br/><br/><br/>

When building a classification tree, either the Gini index or the entropy are typically used to evaluate the quality of a particular split.

\newpage

# Trees vs. Linear Models

Regression and classification trees have a very different feel from the more classical approaches for regression and classification. 

<br/><br/><br/><br/><br/><br/><br/>

Which method is better?

<br/><br/><br/><br/>

## Advantages and Disadvantages of Trees

\newpage

# Bagging

Decision trees suffer from *high variance*.

<br/><br/><br/><br/>

*Bootstrap aggregation* or *bagging* is a general-purpose procedure for reducing the variance of a statistical learning method, particularly useful for trees.

<br/><br/><br/><br/><br/><br/><br/><br/>

So a natural way to reduce the variance is to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions.

<br/><br/><br/><br/><br/><br/>

Of course, this is not practical because we generally do not have access to multiple training sets.

\newpage

While bagging can improve predictions for many regression methods, it's particularly useful for decision trees.

<br/><br/><br/><br/>

These trees are grown deep and not pruned.

<br/><br/><br/><br/>

How can bagging be extended to a classification problem?

<br/><br/><br/><br/>

## Out-of-Bag Error

There is a very straightforward way to estimate the test error of a bagged model, without the need to perform cross-validation.

\newpage

## Interpretation

\newpage

# Random Forests

*Random forests* provide an improvement over bagged trees by a small tweak that decorrelates the trees.

<br/>

As with bagged trees, we build a number of decision trees on bootstrapped training samples.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

In other words, in building a random forest, at each split in the tree, the algorithm is not allowed to consider a majority of the predictors.

<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>

The main difference between bagging and random forests is the choice of predictor subset size $m$.

\newpage

# Boosting

*Boosting* is another approach for improving the prediction results from a decision tree.

<br/><br/><br/>

While bagging involves creating multiple copies of the original training data set using the bootstrap and fitting a separate decision tree on each copy,

<br/><br/><br/><br/><br/><br/>

Boosting does not involve bootstrap sampling, instead each tree is fit on a modified version of the original data set.

\newpage

Boosting has three tuning parameters:

<br/>

1. <br/><br/><br/><br/><br/><br/><br/><br/><br/>

2. <br/><br/><br/><br/><br/><br/><br/><br/><br/>

3. <br/><br/><br/><br/><br/><br/><br/><br/><br/>
